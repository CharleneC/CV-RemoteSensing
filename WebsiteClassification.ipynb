{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bmr3kntbMmD"
      },
      "source": [
        "# Website Classification Notebook\n",
        "This notebook takes data from \"The UK Web Archive & Partners\" and tries to apply Machine Learning methods to classify websites given their title.\n",
        "\n",
        "You will learn about the Pandas python library for data exploration/vizualization and use Sentence Transformers (used with Large Language Models) to see how well they will help for Website Classification\n",
        "\n",
        "To use the GPU in Google Colab: Runtime > Change Runtime type , then select \"Python 3\" and \"T4 GPU\"\n",
        "\n",
        "If you'd like to learn more about the dataset, go [here](https://data.webarchive.org.uk/opendata/ukwa.ds.1/classification/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-VxuHvRcQcD"
      },
      "outputs": [],
      "source": [
        "#import libraries and install one we'll use later on\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install sentence_transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkxZhvJga3t9"
      },
      "outputs": [],
      "source": [
        "#use this bash command to download the dataset\n",
        "!curl -O https://data.webarchive.org.uk/opendata/ukwa.ds.1/classification/classification.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz4p7byQcHC3"
      },
      "outputs": [],
      "source": [
        "#set the file path and open with Pandas\n",
        "website_data_path = './classification.tsv'\n",
        "website_df = pd.read_csv(website_data_path, sep='\\t',header=0, on_bad_lines='skip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2EK2NcjdCWD"
      },
      "outputs": [],
      "source": [
        "#use \".head()\" to view the first 10 rows on the table\n",
        "website_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_az0sKUhkpeA"
      },
      "outputs": [],
      "source": [
        "# Write code here that collects just the \"Primary Category\" column into an array\n",
        "#Hint: website_df['Column_Name'] returns an array of all the values in that column\n",
        "\n",
        "all_categories = #write code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amVnMXDZd1HK"
      },
      "outputs": [],
      "source": [
        "# Answer the following questions:\n",
        "\n",
        "#1: How many websites are in this dataset? Hint: use len() to find how many category labels you collected\n",
        "\n",
        "num_websites = #write code here\n",
        "\n",
        "print('There are this many websites:',num_websites)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#2: How many unique Primary Categories are there? Hint: array.unique() returns another array that removes duplicates\n",
        "unique_categories = #write code here\n",
        "num_unique_primary = len(unique_categories)\n",
        "print('There are this many Primary Categories:',num_unique_primary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#3: How many unique Seconday Categories are there?\n",
        "num_unique_secondary = #write code here\n",
        "print('There are this many Secondary Categories:',num_unique_secondary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-SCdnDvj4oU"
      },
      "outputs": [],
      "source": [
        "#Q4: What are the top three most represented categories found in the dataset?\n",
        "\n",
        "# The following creates a histogram of the labels, use this plot to answer the question.\n",
        "all_categories.value_counts(sort=False).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vRAPJbJYxXv"
      },
      "source": [
        "# Sentence Embeddings\n",
        "Imagine you have a really long book, and you want to understand the main ideas without reading every single word. Sentence embedding is like creating a shortcut for that.\n",
        "\n",
        "In more technical terms, itâ€™s a way to convert a sentence into a list of numbers (a vector) that captures its meaning. Each sentence gets turned into a fixed-length representation that reflects its context and the relationships between words. This makes it easier for computers to compare sentences, find similar ones, or understand their meaning.\n",
        "\n",
        "Think of it like giving every sentence a unique fingerprint that highlights its essence. This helps in tasks like finding relevant information, translating languages, or even summarizing text!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN_8v-tboHsb"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "#grab a pre-trained model that will embed our sentences\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2').cuda()\n",
        "\n",
        "# Sentences are encoded by calling model.encode(). Here we are embedding the sentence \"Arts in Humanities\", the first element in the unique_categories array\n",
        "embedding = model.encode(unique_categories[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtnoLiWUbwLB"
      },
      "outputs": [],
      "source": [
        "# Question 5: How many features are in each embedding? Hint: find the shape of the embedding array with it's .shape attribute\n",
        "\n",
        "num_features = #write code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnJE2Yt9ZePG"
      },
      "source": [
        "We want to compare the Category Embeddings with our Title Embeddings to see if we can automatically classify a website given its title.\n",
        "\n",
        "To test out this theory, we will do the following:\n",
        "- embed each category into its vector representation\n",
        "- for each Website Title, we will embed it and then select the Category Embedding it is closest to.\n",
        "\n",
        "There are many ways to measure \"closeness\", we will try a standard euclidean distance metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDrR7IyCeQnZ"
      },
      "outputs": [],
      "source": [
        "# here is a fuction that finds which row an array is closest to\n",
        "\n",
        "def closest_row_euclidean(A, B, device='cuda'):\n",
        "   # Convert A and B to PyTorch tensors and move to the specified device\n",
        "    A = torch.tensor(A, device=device).reshape(1, -1)  # Reshape A to be 2D (1x384)\n",
        "    B = torch.tensor(B, device=device)  # Convert B to a tensor\n",
        "\n",
        "    # Calculate the Euclidean distances\n",
        "    distances = torch.norm(B - A, dim=1)\n",
        "\n",
        "    # Find the index of the closest row\n",
        "    closest_index = torch.argmin(distances)\n",
        "\n",
        "    # the closest row is B[closest_index], but we only care about the index\n",
        "    return closest_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxkM0q1XoyMP"
      },
      "outputs": [],
      "source": [
        "#Encoding all of our Categories (Labels)\n",
        "\n",
        "#create empty numpy array to hold all embeddings\n",
        "category_embeddings = np.zeros((num_unique_primary,num_features))\n",
        "\n",
        "for i, category in enumerate(unique_categories):\n",
        "  embedding = model.encode(category)\n",
        "  category_embeddings[i,:] = embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mren5UbfciOu"
      },
      "outputs": [],
      "source": [
        "# Write code here to create a dictionary mapping such that:\n",
        "# Arts and Humanities = 0\n",
        "# Business Economy and Industry = 1\n",
        "# Company Websites = 2\n",
        "# ... etc\n",
        "\n",
        "#HINT: Use dict() and zip() together as dict(zip(array1,array2)) where array2 = np.arange(len(unique_categories)))\n",
        "category_mapping = #write code here\n",
        "category_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvYKu2n4pdqA"
      },
      "outputs": [],
      "source": [
        "# Go through each Website Title, get it's sentence embedding, and compare it to the\n",
        "# embeddings of the titles. Label the website with the category it's \"closest\" to\n",
        "\n",
        "predicted_labels = []\n",
        "truth_labels = []\n",
        "\n",
        "for i in tqdm(range(len(website_df))):\n",
        "  title = website_df.iloc[i]['Title']\n",
        "  title_embedding = model.encode(title).astype(float)\n",
        "\n",
        "  predicted_label = closest_row_euclidean(title_embedding,category_embeddings)\n",
        "\n",
        "  predicted_labels.append(predicted_label)\n",
        "\n",
        "  #collect the truth category so we can see how well this classification method works\n",
        "  truth_category = website_df.iloc[i]['Primary Category']\n",
        "  truth_label = category_mapping[truth_category]\n",
        "  truth_labels.append(truth_label)\n",
        "\n",
        "#convert the lists\n",
        "predicted_labels = torch.tensor(predicted_labels)\n",
        "truth_labels = torch.tensor(truth_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDFJPu8HnhXC"
      },
      "outputs": [],
      "source": [
        "# how well did we do? Is this a good method to classify websites?\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(truth_labels, predicted_labels)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
